{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"Final_Cov_Research_Paper.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f=data.drop_duplicates(subset=['Title','Pub_auth','Department','Pub_Year','Paper_Link','Tags','Access_Doc','Abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv(r\"file_name.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                                    3265\n",
       "Engineering & Materials Science                                                                     2109\n",
       "Social Sciences                                                                                     1804\n",
       "Medicine & Life Sciences                                                                            1782\n",
       "Business & Economics                                                                                1052\n",
       "                                                                                                    ... \n",
       "Agriculture & Biology,Business & Economics,Earth & Environmental Sciences,Social Sciences              1\n",
       "Business & Economics,Engineering & Materials Science,Chemical Compounds,Medicine & Life Sciences       1\n",
       "Earth & Environmental Sciences,Engineering & Materials Science,Medicine & Life Sciences                1\n",
       "Arts & Humanities,Social Sciences,Earth & Environmental Sciences,Business & Economics                  1\n",
       "Social Sciences,Agriculture & Biology,Medicine & Life Sciences,Earth & Environmental Sciences          1\n",
       "Name: Tags, Length: 403, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_f['Tags'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Title  \\\n",
      "7806                                 Informal Employment   \n",
      "10594  Seven key decision factors for selecting e-lea...   \n",
      "20525              Associations and Democracy in Algeria   \n",
      "23808  Revisiting the Expected Utility Theory and the...   \n",
      "27391  The Geolinguistics of English as an Academic L...   \n",
      "...                                                  ...   \n",
      "17261  Achieving both high selectivity and current de...   \n",
      "2398   Limerick and the Anglo-Irish economic war 1932...   \n",
      "26694  Projeto Economia Criativa: Dia 3 Manual Para C...   \n",
      "4493   Formation of atomic fluorine anions in 12CaOÂ·7...   \n",
      "23102  The Discourses of Snow Disruption to Business ...   \n",
      "\n",
      "                                                    Tags  \n",
      "7806                                Business & Economics  \n",
      "10594  Social Sciences,Arts & Humanities,Engineering ...  \n",
      "20525     Social Sciences,Earth & Environmental Sciences  \n",
      "23808                                                     \n",
      "27391                  Arts & Humanities,Social Sciences  \n",
      "...                                                  ...  \n",
      "17261                                 Chemical Compounds  \n",
      "2398                   Arts & Humanities,Social Sciences  \n",
      "26694                                    Social Sciences  \n",
      "4493                                 Physics & Astronomy  \n",
      "23102                                    Social Sciences  \n",
      "\n",
      "[14283 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "part_85 = test_data.sample(frac = 0.85)\n",
    "print(part_85)\n",
    "# # Creating dataframe with \n",
    "# # rest of the 25% values\n",
    "# rest_part_25 = df.drop(part_75.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into 85:15 for Classification:\n",
    "part_85.to_csv('file_name.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.Pub_Year = data.Pub_Year.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = data.pivot_table(index = ['Title','Pub_auth','Department','Pub_Year','Paper_Link','Tags','Access_Doc','Abstract'], aggfunc ='size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f=data.drop_duplicates(subset=['Title','Pub_auth','Department','Pub_Year','Paper_Link','Tags','Access_Doc','Abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data_f[['Title','Tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ManualIndex Creation:\n",
    "\n",
    "# a set to check if a word is occuring again\n",
    "s = set()\n",
    "#words are for holding words, count is for how many times the word is occured and indexx represents where the word was taken from\n",
    "words = []\n",
    "count = [0]*99999\n",
    "indexx = [\"\"]*99999\n",
    "#getting the data from the json file to lists\n",
    "title = data_f.Title.tolist()\n",
    "Department = data_f.Department.tolist()\n",
    "Publication = data_f.Pub_Year.tolist()\n",
    "Paper = data_f.Paper_Link.tolist()\n",
    "Tags = data_f.Tags.tolist()\n",
    "Abstract = data_f.Abstract.tolist()\n",
    "Access_doc = data_f.Access_Doc.tolist()\n",
    "authors = data_f.Pub_auth.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(title)):\n",
    "    ws = re.sub(\"[^\\w]\", \" \", title[i]).split()\n",
    "    for wds in ws:\n",
    "        if not wds in stopwords.words():\n",
    "            if (wds in s):\n",
    "                count[words.index(wds)] += 1\n",
    "                indexx[words.index(wds)] = indexx[words.index(wds)] + \" \" + str(i)\n",
    "            else:\n",
    "                s.add(wds)\n",
    "                words.append(wds)\n",
    "                count[words.index(wds)] += 1\n",
    "                indexx[words.index(wds)] = indexx[words.index(wds)] + \" \" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(Department)):\n",
    "    ws = re.sub(\"[^\\w]\", \" \", Department[i]).split()\n",
    "    for wds in ws:\n",
    "        if not wds in stopwords.words():\n",
    "            if (wds in s):\n",
    "                count[words.index(wds)] += 1\n",
    "                indexx[words.index(wds)] = indexx[words.index(wds)] + \" \" + str(i)\n",
    "            else:\n",
    "                s.add(wds)\n",
    "                words.append(wds)\n",
    "                count[words.index(wds)] += 1\n",
    "                indexx[words.index(wds)] = indexx[words.index(wds)] + \" \" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(Publication)):\n",
    "    ws = re.sub(\"[^\\w]\", \" \", Publication[i]).split()\n",
    "    for wds in ws:\n",
    "        if not wds in stopwords.words():\n",
    "            if (wds in s):\n",
    "                count[words.index(wds)] += 1\n",
    "                indexx[words.index(wds)] = indexx[words.index(wds)] + \" \" + str(i)\n",
    "            else:\n",
    "                s.add(wds)\n",
    "                words.append(wds)\n",
    "                count[words.index(wds)] += 1\n",
    "                indexx[words.index(wds)] = indexx[words.index(wds)] + \" \" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(Paper)):\n",
    "    ws = re.sub(\"[^\\w]\", \" \", Paper[i]).split()\n",
    "    for wds in ws:\n",
    "        if not wds in stopwords.words():\n",
    "            if (wds in s):\n",
    "                count[words.index(wds)] += 1\n",
    "                indexx[words.index(wds)] = indexx[words.index(wds)] + \" \" + str(i)\n",
    "            else:\n",
    "                s.add(wds)\n",
    "                words.append(wds)\n",
    "                count[words.index(wds)] += 1\n",
    "                indexx[words.index(wds)] = indexx[words.index(wds)] + \" \" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(Tags)):\n",
    "    ws = re.sub(\"[^\\w]\", \" \", Tags[i]).split()\n",
    "    for wds in ws:\n",
    "        if not wds in stopwords.words():\n",
    "            if (wds in s):\n",
    "                count[words.index(wds)] += 1\n",
    "                indexx[words.index(wds)] = indexx[words.index(wds)] + \" \" + str(i)\n",
    "            else:\n",
    "                s.add(wds)\n",
    "                words.append(wds)\n",
    "                count[words.index(wds)] += 1\n",
    "                indexx[words.index(wds)] = indexx[words.index(wds)] + \" \" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(Access_doc)):\n",
    "    ws = re.sub(\"[^\\w]\", \" \", Access_doc[i]).split()\n",
    "    for wds in ws:\n",
    "        if not wds in stopwords.words():\n",
    "            if (wds in s):\n",
    "                count[words.index(wds)] += 1\n",
    "                indexx[words.index(wds)] = indexx[words.index(wds)] + \" \" + str(i)\n",
    "            else:\n",
    "                s.add(wds)\n",
    "                words.append(wds)\n",
    "                count[words.index(wds)] += 1\n",
    "                indexx[words.index(wds)] = indexx[words.index(wds)] + \" \" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(Abstract)):\n",
    "    ws = re.sub(\"[^\\w]\", \" \", Abstract[i]).split()\n",
    "    for wds in ws:\n",
    "        if not wds in stopwords.words():\n",
    "            if (wds in s):\n",
    "                count[words.index(wds)] += 1\n",
    "                indexx[words.index(wds)] = indexx[words.index(wds)] + \" \" + str(i)\n",
    "            else:\n",
    "                s.add(wds)\n",
    "                words.append(wds)\n",
    "                count[words.index(wds)] += 1\n",
    "                indexx[words.index(wds)] = indexx[words.index(wds)] + \" \" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(authors)):\n",
    "    ws = re.sub(\"[^\\w]\", \" \", authors[i]).split()\n",
    "    for wds in ws:\n",
    "        if not wds in stopwords.words():\n",
    "            if (wds in s):\n",
    "                count[words.index(wds)] += 1\n",
    "                indexx[words.index(wds)] = indexx[words.index(wds)] + \" \" + str(i)\n",
    "            else:\n",
    "                s.add(wds)\n",
    "                words.append(wds)\n",
    "                count[words.index(wds)] += 1\n",
    "                indexx[words.index(wds)] = indexx[words.index(wds)] + \" \" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then creating a inverted indexed csv\n",
    "inverted = pd.DataFrame()\n",
    "inverted['words']=words\n",
    "del count[len(words):]\n",
    "del indexx[len(words):]\n",
    "inverted['count']=count\n",
    "inverted['index']=indexx\n",
    "inverted.to_csv(\"inverted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
